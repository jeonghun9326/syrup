{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(raw_file, 산지공판장_file, 전국도매_file, 품목명, 거래단위, scaler=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    # Load data\n",
    "    raw_data = pd.read_csv(raw_file)\n",
    "    산지공판장 = pd.read_csv(산지공판장_file)\n",
    "    전국도매 = pd.read_csv(전국도매_file)\n",
    "\n",
    "    # 타겟 및 메타데이터 필터 조건 정의\n",
    "    conditions = {\n",
    "    '감자': {\n",
    "        'target': lambda df: (df['품종명'] == '감자 수미') & (df['거래단위'] == '20키로상자') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['감자'], '품종명': ['수미'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['감자'], '품종명': ['수미']}\n",
    "    },\n",
    "    '건고추': {\n",
    "        'target': lambda df: (df['품종명'] == '화건') & (df['거래단위'] == '30 kg') & (df['등급'] == '상품'),\n",
    "        '공판장': None, \n",
    "        '도매': None  \n",
    "    },\n",
    "    '깐마늘(국산)': {\n",
    "        'target': lambda df: (df['거래단위'] == '20 kg') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['마늘'], '품종명': ['깐마늘'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['마늘'], '품종명': ['깐마늘']}\n",
    "    },\n",
    "    '대파': {\n",
    "        'target': lambda df: (df['품종명'] == '대파(일반)') & (df['거래단위'] == '1키로단') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['대파'], '품종명': ['대파(일반)'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['대파'], '품종명': ['대파(일반)']}\n",
    "    },\n",
    "    '무': {\n",
    "        'target': lambda df: (df['거래단위'] == '20키로상자') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['무'], '품종명': ['기타무'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['무'], '품종명': ['무']}\n",
    "    },\n",
    "    '배추': {\n",
    "        'target': lambda df: (df['거래단위'] == '10키로망대') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['배추'], '품종명': ['쌈배추'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['배추'], '품종명': ['배추']}\n",
    "    },\n",
    "    '사과': {\n",
    "        'target': lambda df: (df['품종명'].isin(['홍로', '후지'])) & (df['거래단위'] == '10 개') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['사과'], '품종명': ['후지'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['사과'], '품종명': ['후지']}\n",
    "    },\n",
    "    '상추': {\n",
    "        'target': lambda df: (df['품종명'] == '청') & (df['거래단위'] == '100 g') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['상추'], '품종명': ['청상추'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['상추'], '품종명': ['청상추']}\n",
    "    },\n",
    "    '양파': {\n",
    "        'target': lambda df: (df['품종명'] == '양파') & (df['거래단위'] == '1키로') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['양파'], '품종명': ['기타양파'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['양파'], '품종명': ['양파(일반)']}\n",
    "    },\n",
    "    '배': {\n",
    "        'target': lambda df: (df['품종명'] == '신고') & (df['거래단위'] == '10 개') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['배'], '품종명': ['신고'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['배'], '품종명': ['신고']}\n",
    "    }\n",
    "    }\n",
    " \n",
    "    def adjust_prices(data, conversion_factor):\n",
    "        price_columns = ['평균가(원/kg)', '최저가(원/kg)', '최고가(원/kg)', '중간가(원/kg)']\n",
    "        for col in price_columns:\n",
    "            if col in data.columns:\n",
    "                data[col] = data[col] * conversion_factor\n",
    "        return data\n",
    "\n",
    "    # Adjust prices for 산지공판장 and 전국도매\n",
    "    if 품목명 in 거래단위:\n",
    "        for 단위, factor in 거래단위[품목명].items():\n",
    "            산지공판장 = adjust_prices(산지공판장, factor)\n",
    "            전국도매 = adjust_prices(전국도매, factor)\n",
    "\n",
    "    # Target filtering\n",
    "    if 품목명 in conditions and 'target' in conditions[품목명]:\n",
    "        raw_품목 = raw_data[raw_data['품목명'] == 품목명]\n",
    "        filtered_data = raw_품목[conditions[품목명]['target'](raw_품목)]\n",
    "    else:\n",
    "        filtered_data = raw_data[raw_data['품목명'] == 품목명]\n",
    "\n",
    "    # 필터링된 산지공판장 및 도매 데이터\n",
    "    if 품목명 in conditions:\n",
    "        if conditions[품목명].get('공판장'):\n",
    "            for key, values in conditions[품목명]['공판장'].items():\n",
    "                산지공판장 = 산지공판장[산지공판장[key].isin(values)]\n",
    "        if conditions[품목명].get('도매'):\n",
    "            for key, values in conditions[품목명]['도매'].items():\n",
    "                전국도매 = 전국도매[전국도매[key].isin(values)]\n",
    "\n",
    "    # Merge data\n",
    "    filtered_공판장 = 산지공판장.add_prefix('공판장_').rename(columns={'공판장_시점': '시점'})\n",
    "    filtered_도매 = 전국도매.add_prefix('도매_').rename(columns={'도매_시점': '시점'})\n",
    "    filtered_data = filtered_data.merge(filtered_공판장, on='시점', how='left')\n",
    "    filtered_data = filtered_data.merge(filtered_도매, on='시점', how='left')\n",
    "\n",
    "    # 파생변수 생성\n",
    "    if {'최고가(원/kg)', '중간가(원/kg)', '최저가(원/kg)'}.issubset(filtered_data.columns):\n",
    "        filtered_data['변동폭(최고_최저)'] = filtered_data['최고가(원/kg)'] - filtered_data['최저가(원/kg)']\n",
    "        filtered_data['변동폭(평균_중간)'] = filtered_data['평균가(원/kg)'] - filtered_data['중간가(원/kg)']\n",
    "\n",
    "    # 표준편차 계산\n",
    "    filtered_data['시점_날짜'] = pd.to_datetime(filtered_data['시점'], errors='coerce')\n",
    "    if not filtered_data['시점_날짜'].isna().all():\n",
    "        monthly_std = filtered_data.groupby(filtered_data['시점_날짜'].dt.to_period('M'))[\n",
    "            ['최고가(원/kg)', '최저가(원/kg)', '평균가(원/kg)']\n",
    "        ].std().add_prefix('월_표준편차_')\n",
    "        filtered_data = filtered_data.merge(monthly_std, left_on=filtered_data['시점_날짜'].dt.to_period('M'), right_index=True, how='left')\n",
    "\n",
    "    # 품목별 선형회귀분석\n",
    "    if {'최고가(원/kg)', '중간가(원/kg)', '최저가(원/kg)', '평균가(원/kg)'}.issubset(filtered_data.columns):\n",
    "        regression_model = LinearRegression()\n",
    "        valid_rows = filtered_data.dropna(subset=['최고가(원/kg)', '중간가(원/kg)', '최저가(원/kg)', '평균가(원/kg)'])\n",
    "        X = valid_rows[['최고가(원/kg)', '중간가(원/kg)', '최저가(원/kg)']]\n",
    "        y = valid_rows['평균가(원/kg)']\n",
    "        if not X.empty:\n",
    "            regression_model.fit(X, y)\n",
    "            filtered_data['월_시세'] = (\n",
    "                regression_model.coef_[0] * filtered_data['최고가(원/kg)'] +\n",
    "                regression_model.coef_[1] * filtered_data['중간가(원/kg)'] +\n",
    "                regression_model.coef_[2] * filtered_data['최저가(원/kg)']\n",
    "            )\n",
    "        else:\n",
    "            filtered_data['월_시세'] = np.nan\n",
    "\n",
    "    # 데이터 타입 최적화\n",
    "    numeric_columns = filtered_data.select_dtypes(include=[np.number]).columns\n",
    "    filtered_data[numeric_columns] = filtered_data[numeric_columns].fillna(0).astype('float32')\n",
    "\n",
    "    # 정규화\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        filtered_data[numeric_columns] = scaler.fit_transform(filtered_data[numeric_columns])\n",
    "    else:\n",
    "        filtered_data[numeric_columns] = scaler.transform(filtered_data[numeric_columns])\n",
    "\n",
    "    return filtered_data, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_data(raw_file, 산지공판장_file, 전국도매_file, 품목명, 거래단위, scaler=None):\n",
    "#     import pandas as pd\n",
    "#     import numpy as np\n",
    "#     from sklearn.preprocessing import MinMaxScaler\n",
    "#     from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#     # Load data\n",
    "#     raw_data = pd.read_csv(raw_file)\n",
    "#     산지공판장 = pd.read_csv(산지공판장_file)\n",
    "#     전국도매 = pd.read_csv(전국도매_file)\n",
    "\n",
    "#     # 타겟 및 메타데이터 필터 조건 정의\n",
    "#     conditions = {\n",
    "#     '감자': {\n",
    "#         'target': lambda df: (df['품종명'] == '감자 수미') & (df['거래단위'] == '20키로상자') & (df['등급'] == '상'),\n",
    "#         '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['감자'], '품종명': ['수미'], '등급명': ['상']},\n",
    "#         '도매': {'시장명': ['*전국도매시장'], '품목명': ['감자'], '품종명': ['수미']}\n",
    "#     },\n",
    "#     '건고추': {\n",
    "#         'target': lambda df: (df['품종명'] == '화건') & (df['거래단위'] == '30 kg') & (df['등급'] == '상품'),\n",
    "#         '공판장': None, \n",
    "#         '도매': None  \n",
    "#     },\n",
    "#     '깐마늘(국산)': {\n",
    "#         'target': lambda df: (df['거래단위'] == '20 kg') & (df['등급'] == '상품'),\n",
    "#         '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['마늘'], '품종명': ['깐마늘'], '등급명': ['상']},\n",
    "#         '도매': {'시장명': ['*전국도매시장'], '품목명': ['마늘'], '품종명': ['깐마늘']}\n",
    "#     },\n",
    "#     '대파': {\n",
    "#         'target': lambda df: (df['품종명'] == '대파(일반)') & (df['거래단위'] == '1키로단') & (df['등급'] == '상'),\n",
    "#         '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['대파'], '품종명': ['대파(일반)'], '등급명': ['상']},\n",
    "#         '도매': {'시장명': ['*전국도매시장'], '품목명': ['대파'], '품종명': ['대파(일반)']}\n",
    "#     },\n",
    "#     '무': {\n",
    "#         'target': lambda df: (df['거래단위'] == '20키로상자') & (df['등급'] == '상'),\n",
    "#         '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['무'], '품종명': ['기타무'], '등급명': ['상']},\n",
    "#         '도매': {'시장명': ['*전국도매시장'], '품목명': ['무'], '품종명': ['무']}\n",
    "#     },\n",
    "#     '배추': {\n",
    "#         'target': lambda df: (df['거래단위'] == '10키로망대') & (df['등급'] == '상'),\n",
    "#         '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['배추'], '품종명': ['쌈배추'], '등급명': ['상']},\n",
    "#         '도매': {'시장명': ['*전국도매시장'], '품목명': ['배추'], '품종명': ['배추']}\n",
    "#     },\n",
    "#     '사과': {\n",
    "#         'target': lambda df: (df['품종명'].isin(['홍로', '후지'])) & (df['거래단위'] == '10 개') & (df['등급'] == '상품'),\n",
    "#         '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['사과'], '품종명': ['후지'], '등급명': ['상']},\n",
    "#         '도매': {'시장명': ['*전국도매시장'], '품목명': ['사과'], '품종명': ['후지']}\n",
    "#     },\n",
    "#     '상추': {\n",
    "#         'target': lambda df: (df['품종명'] == '청') & (df['거래단위'] == '100 g') & (df['등급'] == '상품'),\n",
    "#         '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['상추'], '품종명': ['청상추'], '등급명': ['상']},\n",
    "#         '도매': {'시장명': ['*전국도매시장'], '품목명': ['상추'], '품종명': ['청상추']}\n",
    "#     },\n",
    "#     '양파': {\n",
    "#         'target': lambda df: (df['품종명'] == '양파') & (df['거래단위'] == '1키로') & (df['등급'] == '상'),\n",
    "#         '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['양파'], '품종명': ['기타양파'], '등급명': ['상']},\n",
    "#         '도매': {'시장명': ['*전국도매시장'], '품목명': ['양파'], '품종명': ['양파(일반)']}\n",
    "#     },\n",
    "#     '배': {\n",
    "#         'target': lambda df: (df['품종명'] == '신고') & (df['거래단위'] == '10 개') & (df['등급'] == '상품'),\n",
    "#         '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['배'], '품종명': ['신고'], '등급명': ['상']},\n",
    "#         '도매': {'시장명': ['*전국도매시장'], '품목명': ['배'], '품종명': ['신고']}\n",
    "#     }\n",
    "#     }\n",
    "#     # 거래단위 조정 함수\n",
    "#     def adjust_prices(data, conversion_factor):\n",
    "#         price_columns = ['평균가(원/kg)', '최저가(원/kg)', '최고가(원/kg)', '중간가(원/kg)']\n",
    "#         for col in price_columns:\n",
    "#             if col in data.columns:\n",
    "#                 data[col] = data[col] * conversion_factor\n",
    "#         return data\n",
    "\n",
    "#     # Adjust prices for 산지공판장\n",
    "#     if 품목명 in 거래단위:\n",
    "#         for 단위, factor in 거래단위[품목명].items():\n",
    "#             산지공판장 = adjust_prices(산지공판장, factor)\n",
    "#             전국도매 = adjust_prices(전국도매, factor)\n",
    "\n",
    "#     # Target filtering\n",
    "#     raw_품목 = raw_data[raw_data['품목명'] == 품목명]\n",
    "#     filtered_data = raw_품목.copy()\n",
    "\n",
    "#     # Merge 산지공판장 and 전국도매 with the raw data\n",
    "#     filtered_공판장 = 산지공판장.add_prefix('공판장_').rename(columns={'공판장_시점': '시점'})\n",
    "#     filtered_도매 = 전국도매.add_prefix('도매_').rename(columns={'도매_시점': '시점'})\n",
    "#     filtered_data = filtered_data.merge(filtered_공판장, on='시점', how='left')\n",
    "#     filtered_data = filtered_data.merge(filtered_도매, on='시점', how='left')\n",
    "\n",
    "#     # 파생변수 생성\n",
    "#     if {'최고가(원/kg)', '중간가(원/kg)', '최저가(원/kg)'}.issubset(filtered_data.columns):\n",
    "#         filtered_data['변동폭(최고_최저)'] = filtered_data['최고가(원/kg)'] - filtered_data['최저가(원/kg)']\n",
    "#         filtered_data['변동폭(평균_중간)'] = filtered_data['평균가(원/kg)'] - filtered_data['중간가(원/kg)']\n",
    "\n",
    "#     # 표준편차 계산\n",
    "#     filtered_data['시점_날짜'] = pd.to_datetime(filtered_data['시점'], errors='coerce')\n",
    "#     if not filtered_data['시점_날짜'].isna().all():\n",
    "#         monthly_std = filtered_data.groupby(filtered_data['시점_날짜'].dt.to_period('M'))[\n",
    "#             ['최고가(원/kg)', '최저가(원/kg)', '평균가(원/kg)']\n",
    "#         ].std().add_prefix('월_표준편차_')\n",
    "#         filtered_data = filtered_data.merge(monthly_std, left_on=filtered_data['시점_날짜'].dt.to_period('M'), right_index=True, how='left')\n",
    "\n",
    "#     # 품목별 선형회귀분석\n",
    "#     if {'최고가(원/kg)', '중간가(원/kg)', '최저가(원/kg)', '평균가(원/kg)'}.issubset(filtered_data.columns):\n",
    "#         regression_model = LinearRegression()\n",
    "#         valid_rows = filtered_data.dropna(subset=['최고가(원/kg)', '중간가(원/kg)', '최저가(원/kg)', '평균가(원/kg)'])\n",
    "#         X = valid_rows[['최고가(원/kg)', '중간가(원/kg)', '최저가(원/kg)']]\n",
    "#         y = valid_rows['평균가(원/kg)']\n",
    "#         if not X.empty:\n",
    "#             regression_model.fit(X, y)\n",
    "#             filtered_data['월_시세'] = (\n",
    "#                 regression_model.coef_[0] * filtered_data['최고가(원/kg)'] +\n",
    "#                 regression_model.coef_[1] * filtered_data['중간가(원/kg)'] +\n",
    "#                 regression_model.coef_[2] * filtered_data['최저가(원/kg)']\n",
    "#             )\n",
    "#         else:\n",
    "#             filtered_data['월_시세'] = np.nan\n",
    "\n",
    "#     # Normalize numeric columns\n",
    "#     numeric_columns = filtered_data.select_dtypes(include=[np.number]).columns\n",
    "#     if scaler is None:\n",
    "#         scaler = MinMaxScaler()\n",
    "#         filtered_data[numeric_columns] = scaler.fit_transform(filtered_data[numeric_columns])\n",
    "#     else:\n",
    "#         filtered_data[numeric_columns] = scaler.transform(filtered_data[numeric_columns])\n",
    "\n",
    "#     return filtered_data, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('C:/Users/medici/Team_Pro/syrup/train.csv')\n",
    "df_apply = pd.read_csv('C:/Users/medici/Team_Pro/syrup/TRAIN_산지공판장_2018-2021.csv')\n",
    "df_market = pd.read_csv('C:/Users/medici/Team_Pro/syrup/TRAIN_전국도매_2018-2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "품목별 전처리 및 모델 학습 -> 건고추:   0%|          | 0/10 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 9.97 GiB for an array with shape (9, 148691282) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m pbar_outer\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m품목별 전처리 및 모델 학습 -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m품목명\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# 데이터 전처리\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m train_data, scaler \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/medici/pybasic/train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/medici/pybasic/TRAIN_산지공판장_2018-2021.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/medici/pybasic/TRAIN_전국도매_2018-2021.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m품목명\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m거래단위\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m 품목별_scalers[품목명] \u001b[38;5;241m=\u001b[39m scaler\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Feature/Target 분리\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 99\u001b[0m, in \u001b[0;36mprocess_data\u001b[1;34m(raw_file, 산지공판장_file, 전국도매_file, 품목명, 거래단위, scaler)\u001b[0m\n\u001b[0;32m     97\u001b[0m filtered_도매 \u001b[38;5;241m=\u001b[39m 전국도매\u001b[38;5;241m.\u001b[39madd_prefix(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m도매_\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m도매_시점\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m시점\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     98\u001b[0m filtered_data \u001b[38;5;241m=\u001b[39m filtered_data\u001b[38;5;241m.\u001b[39mmerge(filtered_공판장, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m시점\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 99\u001b[0m filtered_data \u001b[38;5;241m=\u001b[39m \u001b[43mfiltered_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_도매\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m시점\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# 파생변수 생성\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m최고가(원/kg)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m중간가(원/kg)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m최저가(원/kg)\u001b[39m\u001b[38;5;124m'\u001b[39m}\u001b[38;5;241m.\u001b[39missubset(filtered_data\u001b[38;5;241m.\u001b[39mcolumns):\n",
      "File \u001b[1;32mc:\\Users\\medici\\miniconda3\\envs\\syrup\\lib\\site-packages\\pandas\\core\\frame.py:10832\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10813\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m  10814\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m  10815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10828\u001b[0m     validate: MergeValidate \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m  10829\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m  10830\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[1;32m> 10832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10833\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10841\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10842\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10846\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\medici\\miniconda3\\envs\\syrup\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:184\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    171\u001b[0m         left_df,\n\u001b[0;32m    172\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\medici\\miniconda3\\envs\\syrup\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:888\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[0;32m    886\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_info()\n\u001b[1;32m--> 888\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_and_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    891\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n",
      "File \u001b[1;32mc:\\Users\\medici\\miniconda3\\envs\\syrup\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:848\u001b[0m, in \u001b[0;36m_MergeOperation._reindex_and_concat\u001b[1;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[0;32m    840\u001b[0m llabels, rlabels \u001b[38;5;241m=\u001b[39m _items_overlap_with_suffix(\n\u001b[0;32m    841\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft\u001b[38;5;241m.\u001b[39m_info_axis, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright\u001b[38;5;241m.\u001b[39m_info_axis, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuffixes\n\u001b[0;32m    842\u001b[0m )\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_range_indexer(left_indexer, \u001b[38;5;28mlen\u001b[39m(left)):\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;66;03m# Pinning the index here (and in the right code just below) is not\u001b[39;00m\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;66;03m#  necessary, but makes the `.take` more performant if we have e.g.\u001b[39;00m\n\u001b[0;32m    847\u001b[0m     \u001b[38;5;66;03m#  a MultiIndex for left.index.\u001b[39;00m\n\u001b[1;32m--> 848\u001b[0m     lmgr \u001b[38;5;241m=\u001b[39m \u001b[43mleft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m     left \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(lmgr, axes\u001b[38;5;241m=\u001b[39mlmgr\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m    858\u001b[0m left\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m join_index\n",
      "File \u001b[1;32mc:\\Users\\medici\\miniconda3\\envs\\syrup\\lib\\site-packages\\pandas\\core\\internals\\managers.py:687\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[0;32m    681\u001b[0m         indexer,\n\u001b[0;32m    682\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m    683\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[0;32m    684\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[0;32m    685\u001b[0m     )\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m    689\u001b[0m             indexer,\n\u001b[0;32m    690\u001b[0m             axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    691\u001b[0m             fill_value\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    692\u001b[0m                 fill_value \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mfill_value\n\u001b[0;32m    693\u001b[0m             ),\n\u001b[0;32m    694\u001b[0m         )\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    696\u001b[0m     ]\n\u001b[0;32m    698\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m    699\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[1;32mc:\\Users\\medici\\miniconda3\\envs\\syrup\\lib\\site-packages\\pandas\\core\\internals\\managers.py:688\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[0;32m    681\u001b[0m         indexer,\n\u001b[0;32m    682\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m    683\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[0;32m    684\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[0;32m    685\u001b[0m     )\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 688\u001b[0m         \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    696\u001b[0m     ]\n\u001b[0;32m    698\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m    699\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[1;32mc:\\Users\\medici\\miniconda3\\envs\\syrup\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\medici\\miniconda3\\envs\\syrup\\lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\medici\\miniconda3\\envs\\syrup\\lib\\site-packages\\pandas\\core\\array_algos\\take.py:157\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    155\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[0;32m    161\u001b[0m )\n\u001b[0;32m    162\u001b[0m func(arr, indexer, out, fill_value)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 9.97 GiB for an array with shape (9, 148691282) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 품목별 데이터 처리 및 XGBoost 학습\n",
    "품목별_predictions = {}\n",
    "품목별_scalers = {}\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# 품목 리스트와 거래단위 설정\n",
    "품목_리스트 = ['건고추', '사과', '감자', '배', '깐마늘(국산)', '무', '상추', '배추', '양파', '대파']\n",
    "거래단위 = {\n",
    "    '감자': 20, '깐마늘(국산)': 20, '대파': 1, '무': 20,\n",
    "    '배추': 10, '사과': 3, '배': 5, '상추': 0.1, '양파': 1\n",
    "}\n",
    "\n",
    "# Progress bar for 품목\n",
    "pbar_outer = tqdm(품목_리스트, desc=\"품목 처리 중\", position=0)\n",
    "for 품목명 in pbar_outer:\n",
    "    pbar_outer.set_description(f\"품목별 전처리 및 모델 학습 -> {품목명}\")\n",
    "    \n",
    "    # 데이터 전처리\n",
    "    train_data, scaler = process_data(\n",
    "        \"C:/Users/medici/pybasic/train.csv\",\n",
    "        \"C:/Users/medici/pybasic/TRAIN_산지공판장_2018-2021.csv\",\n",
    "        \"C:/Users/medici/pybasic/TRAIN_전국도매_2018-2021.csv\",\n",
    "        품목명, 거래단위\n",
    "    )\n",
    "    품목별_scalers[품목명] = scaler\n",
    "    \n",
    "    # Feature/Target 분리\n",
    "    features = ['최고가(원/kg)', '최저가(원/kg)', '중간가(원/kg)', '변동폭(최고_최저)', '변동폭(평균_중간)', '월_시세']\n",
    "    target = '평균가격(원)'\n",
    "    \n",
    "    if not set(features).issubset(train_data.columns) or target not in train_data.columns:\n",
    "        print(f\"필수 컬럼이 누락되었습니다: {품목명}\")\n",
    "        continue\n",
    "    \n",
    "    X = train_data[features].dropna()\n",
    "    y = train_data[target].loc[X.index]\n",
    "    \n",
    "    if X.empty or y.empty:\n",
    "        print(f\"데이터가 비어 있습니다: {품목명}\")\n",
    "        continue\n",
    "    \n",
    "    # Train/Test Split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # XGBoost 모델 초기화\n",
    "    model = XGBRegressor(n_estimators=50, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "    \n",
    "    # 모델 학습\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose=False)\n",
    "    \n",
    "    # 모델 저장\n",
    "    joblib.dump(model, f'models/best_model_{품목명}.pkl')\n",
    "    \n",
    "    # 성능 평가\n",
    "    val_predictions = model.predict(X_val)\n",
    "    val_mae = mean_absolute_error(y_val, val_predictions)\n",
    "    print(f\"{품목명} - Validation MAE: {val_mae:.4f}\")\n",
    "    \n",
    "    # 테스트 파일 추론\n",
    "    품목_predictions = []\n",
    "    pbar_inner = tqdm(range(25), desc=\"테스트 파일 추론 중\", position=1, leave=False)\n",
    "    for i in pbar_inner:\n",
    "        test_file = f\"C:/Users/medici/pybasic/test/TEST_{i:02d}.csv\"\n",
    "        산지공판장_file = f\"C:/Users/medici/pybasic/test/meta/TEST_산지공판장_{i:02d}.csv\"\n",
    "        전국도매_file = f\"C:/Users/medici/pybasic/test/meta/TEST_전국도매_{i:02d}.csv\"\n",
    "        \n",
    "        test_data, _ = process_data(test_file, 산지공판장_file, 전국도매_file, 품목명, scaler=품목별_scalers[품목명])\n",
    "        test_features = test_data[features].dropna()\n",
    "        \n",
    "        if test_features.empty:\n",
    "            print(f\"테스트 데이터가 비어 있습니다: {test_file}\")\n",
    "            continue\n",
    "        \n",
    "        # 예측 수행\n",
    "        predictions = model.predict(test_features)\n",
    "        품목_predictions.extend(predictions)\n",
    "    \n",
    "    품목별_predictions[품목명] = 품목_predictions\n",
    "    pbar_outer.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "품목 처리 중:   0%|          | 0/10 [00:00<?, ?it/s]C:\\Users\\medici\\AppData\\Local\\Temp\\ipykernel_25852\\523808611.py:123: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  filtered_data['시점_날짜'] = pd.to_datetime(filtered_data['시점'], errors='coerce')\n",
      "품목 처리 중:   0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '감자'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 191\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[0;32m    190\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m--> 191\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# 모델 저장\u001b[39;00m\n\u001b[0;32m    194\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(model, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/best_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m품목명\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\medici\\miniconda3\\envs\\syrup\\lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\medici\\miniconda3\\envs\\syrup\\lib\\site-packages\\xgboost\\sklearn.py:1081\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbosity):\n\u001b[0;32m   1080\u001b[0m     evals_result: TrainingCallback\u001b[38;5;241m.\u001b[39mEvalsLog \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1081\u001b[0m     train_dmatrix, evals \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_evaluation_matrices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1090\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1093\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1094\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_qid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_dmatrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1099\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "File \u001b[1;32mc:\\Users\\medici\\miniconda3\\envs\\syrup\\lib\\site-packages\\xgboost\\sklearn.py:596\u001b[0m, in \u001b[0;36m_wrap_evaluation_matrices\u001b[1;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_evaluation_matrices\u001b[39m(\n\u001b[0;32m    577\u001b[0m     missing: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[0;32m    578\u001b[0m     X: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    592\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[0;32m    593\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, List[Tuple[Any, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;124;03m    way.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 596\u001b[0m     train_dmatrix \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dmatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m     n_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_set \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eval_set)\n\u001b[0;32m    612\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_or_none\u001b[39m(meta: Optional[Sequence], name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence:\n",
      "File \u001b[1;32mc:\\Users\\medici\\miniconda3\\envs\\syrup\\lib\\site-packages\\xgboost\\sklearn.py:1003\u001b[0m, in \u001b[0;36mXGBModel._create_dmatrix\u001b[1;34m(self, ref, **kwargs)\u001b[0m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _can_use_qdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_method) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbooster \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1002\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1003\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m QuantileDMatrix(\n\u001b[0;32m   1004\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, ref\u001b[38;5;241m=\u001b[39mref, nthread\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, max_bin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_bin\n\u001b[0;32m   1005\u001b[0m         )\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\medici\\miniconda3\\envs\\syrup\\lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\medici\\miniconda3\\envs\\syrup\\lib\\site-packages\\xgboost\\core.py:1573\u001b[0m, in \u001b[0;36mQuantileDMatrix.__init__\u001b[1;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, data_split_mode)\u001b[0m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m   1554\u001b[0m         info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1566\u001b[0m         )\n\u001b[0;32m   1567\u001b[0m     ):\n\u001b[0;32m   1568\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1569\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf data iterator is used as input, data like label should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1570\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified as batch argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1571\u001b[0m         )\n\u001b[1;32m-> 1573\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1587\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\medici\\miniconda3\\envs\\syrup\\lib\\site-packages\\xgboost\\core.py:1632\u001b[0m, in \u001b[0;36mQuantileDMatrix._init\u001b[1;34m(self, data, ref, enable_categorical, **meta)\u001b[0m\n\u001b[0;32m   1620\u001b[0m config \u001b[38;5;241m=\u001b[39m make_jcargs(\n\u001b[0;32m   1621\u001b[0m     nthread\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnthread, missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing, max_bin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_bin\n\u001b[0;32m   1622\u001b[0m )\n\u001b[0;32m   1623\u001b[0m ret \u001b[38;5;241m=\u001b[39m _LIB\u001b[38;5;241m.\u001b[39mXGQuantileDMatrixCreateFromCallback(\n\u001b[0;32m   1624\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1625\u001b[0m     it\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1630\u001b[0m     ctypes\u001b[38;5;241m.\u001b[39mbyref(handle),\n\u001b[0;32m   1631\u001b[0m )\n\u001b[1;32m-> 1632\u001b[0m \u001b[43mit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;66;03m# delay check_call to throw intermediate exception first\u001b[39;00m\n\u001b[0;32m   1634\u001b[0m _check_call(ret)\n",
      "File \u001b[1;32mc:\\Users\\medici\\miniconda3\\envs\\syrup\\lib\\site-packages\\xgboost\\core.py:569\u001b[0m, in \u001b[0;36mDataIter.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    567\u001b[0m exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\medici\\miniconda3\\envs\\syrup\\lib\\site-packages\\xgboost\\core.py:550\u001b[0m, in \u001b[0;36mDataIter._handle_exception\u001b[1;34m(self, fn, dft_ret)\u001b[0m\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dft_ret\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;66;03m# Defer the exception in order to return 0 and stop the iteration.\u001b[39;00m\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;66;03m# Exception inside a ctype callback function has no effect except\u001b[39;00m\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;66;03m# for printing to stderr (doesn't stop the execution).\u001b[39;00m\n\u001b[0;32m    555\u001b[0m     tb \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\medici\\miniconda3\\envs\\syrup\\lib\\site-packages\\xgboost\\core.py:637\u001b[0m, in \u001b[0;36mDataIter._next_wrapper.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temporary_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m--> 637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_exception(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\medici\\miniconda3\\envs\\syrup\\lib\\site-packages\\xgboost\\data.py:1388\u001b[0m, in \u001b[0;36mSingleBatchInternalIter.next\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m   1386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mit \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1388\u001b[0m input_data(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\medici\\miniconda3\\envs\\syrup\\lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\medici\\miniconda3\\envs\\syrup\\lib\\site-packages\\xgboost\\core.py:617\u001b[0m, in \u001b[0;36mDataIter._next_wrapper.<locals>.input_data\u001b[1;34m(data, feature_names, feature_types, **kwargs)\u001b[0m\n\u001b[0;32m    615\u001b[0m     new, cat_codes, feature_names, feature_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temporary_data\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 617\u001b[0m     new, cat_codes, feature_names, feature_types \u001b[38;5;241m=\u001b[39m \u001b[43m_proxy_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_enable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;66;03m# Stage the data, meta info are copied inside C++ MetaInfo.\u001b[39;00m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temporary_data \u001b[38;5;241m=\u001b[39m (new, cat_codes, feature_names, feature_types)\n",
      "File \u001b[1;32mc:\\Users\\medici\\miniconda3\\envs\\syrup\\lib\\site-packages\\xgboost\\data.py:1413\u001b[0m, in \u001b[0;36m_proxy_transform\u001b[1;34m(data, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[0;32m   1411\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data)\n\u001b[0;32m   1412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_np_array_like(data):\n\u001b[1;32m-> 1413\u001b[0m     data, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_np_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data, \u001b[38;5;28;01mNone\u001b[39;00m, feature_names, feature_types\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_scipy_csr(data):\n",
      "File \u001b[1;32mc:\\Users\\medici\\miniconda3\\envs\\syrup\\lib\\site-packages\\xgboost\\data.py:224\u001b[0m, in \u001b[0;36m_ensure_np_dtype\u001b[1;34m(data, dtype)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _array_hasobject(data) \u001b[38;5;129;01mor\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m [np\u001b[38;5;241m.\u001b[39mfloat16, np\u001b[38;5;241m.\u001b[39mbool_]:\n\u001b[0;32m    223\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39maligned:\n\u001b[0;32m    226\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrequire(data, requirements\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '감자'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# 타겟 조건\n",
    "conditions = {\n",
    "    '감자': {\n",
    "        'target': lambda df: (df['품종명'] == '감자 수미') & (df['거래단위'] == '20키로상자') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['감자'], '품종명': ['수미'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['감자'], '품종명': ['수미']}\n",
    "    },\n",
    "    '건고추': {\n",
    "        'target': lambda df: (df['품종명'] == '화건') & (df['거래단위'] == '30 kg') & (df['등급'] == '상품'),\n",
    "        '공판장': None, \n",
    "        '도매': None  \n",
    "    },\n",
    "    '깐마늘(국산)': {\n",
    "        'target': lambda df: (df['거래단위'] == '20 kg') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['마늘'], '품종명': ['깐마늘'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['마늘'], '품종명': ['깐마늘']}\n",
    "    },\n",
    "    '대파': {\n",
    "        'target': lambda df: (df['품종명'] == '대파(일반)') & (df['거래단위'] == '1키로단') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['대파'], '품종명': ['대파(일반)'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['대파'], '품종명': ['대파(일반)']}\n",
    "    },\n",
    "    '무': {\n",
    "        'target': lambda df: (df['거래단위'] == '20키로상자') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['무'], '품종명': ['기타무'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['무'], '품종명': ['무']}\n",
    "    },\n",
    "    '배추': {\n",
    "        'target': lambda df: (df['거래단위'] == '10키로망대') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['배추'], '품종명': ['쌈배추'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['배추'], '품종명': ['배추']}\n",
    "    },\n",
    "    '사과': {\n",
    "        'target': lambda df: (df['품종명'].isin(['홍로', '후지'])) & (df['거래단위'] == '10 개') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['사과'], '품종명': ['후지'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['사과'], '품종명': ['후지']}\n",
    "    },\n",
    "    '상추': {\n",
    "        'target': lambda df: (df['품종명'] == '청') & (df['거래단위'] == '100 g') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['상추'], '품종명': ['청상추'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['상추'], '품종명': ['청상추']}\n",
    "    },\n",
    "    '양파': {\n",
    "        'target': lambda df: (df['품종명'] == '양파') & (df['거래단위'] == '1키로') & (df['등급'] == '상'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['양파'], '품종명': ['기타양파'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['양파'], '품종명': ['양파(일반)']}\n",
    "    },\n",
    "    '배': {\n",
    "        'target': lambda df: (df['품종명'] == '신고') & (df['거래단위'] == '10 개') & (df['등급'] == '상품'),\n",
    "        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['배'], '품종명': ['신고'], '등급명': ['상']},\n",
    "        '도매': {'시장명': ['*전국도매시장'], '품목명': ['배'], '품종명': ['신고']}\n",
    "    }\n",
    "    }\n",
    "# 데이터 처리 함수\n",
    "def process_data(raw_file, 산지공판장_file, 전국도매_file, 품목명, 거래단위, scaler=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    # Load data\n",
    "    raw_data = pd.read_csv(raw_file)\n",
    "    산지공판장 = pd.read_csv(산지공판장_file)\n",
    "    전국도매 = pd.read_csv(전국도매_file)\n",
    " \n",
    "    def adjust_prices(data, conversion_factor):\n",
    "        price_columns = ['평균가(원/kg)', '최저가(원/kg)', '최고가(원/kg)', '중간가(원/kg)']\n",
    "        for col in price_columns:\n",
    "            if col in data.columns:\n",
    "                data[col] = data[col] * conversion_factor\n",
    "        return data\n",
    "\n",
    "    # Adjust prices for 산지공판장 and 전국도매\n",
    "    # if 품목명 in 거래단위:\n",
    "    #     for 단위, factor in 거래단위[품목명]():\n",
    "    #         산지공판장 = adjust_prices(산지공판장, factor)\n",
    "    #         전국도매 = adjust_prices(전국도매, factor)\n",
    "    if 품목명 in 거래단위:\n",
    "        factor = 거래단위[품목명]\n",
    "        산지공판장 = adjust_prices(산지공판장, factor)\n",
    "        전국도매 = adjust_prices(전국도매, factor)\n",
    "\n",
    "    # Target filtering\n",
    "    if 품목명 in conditions and 'target' in conditions[품목명]:\n",
    "        raw_품목 = raw_data[raw_data['품목명'] == 품목명]\n",
    "        filtered_data = raw_품목[conditions[품목명]['target'](raw_품목)]\n",
    "    else:\n",
    "        filtered_data = raw_data[raw_data['품목명'] == 품목명]\n",
    "\n",
    "    # 필터링된 산지공판장 및 도매 데이터\n",
    "    if 품목명 in conditions:\n",
    "        if conditions[품목명].get('공판장'):\n",
    "            for key, values in conditions[품목명]['공판장'].items():\n",
    "                산지공판장 = 산지공판장[산지공판장[key].isin(values)]\n",
    "        if conditions[품목명].get('도매'):\n",
    "            for key, values in conditions[품목명]['도매'].items():\n",
    "                전국도매 = 전국도매[전국도매[key].isin(values)]\n",
    "\n",
    "    # Merge data\n",
    "    filtered_공판장 = 산지공판장.add_prefix('공판장_').rename(columns={'공판장_시점': '시점'})\n",
    "    filtered_도매 = 전국도매.add_prefix('도매_').rename(columns={'도매_시점': '시점'})\n",
    "    filtered_data = filtered_data.merge(filtered_공판장, on='시점', how='left')\n",
    "    filtered_data = filtered_data.merge(filtered_도매, on='시점', how='left')\n",
    "\n",
    "    # 파생변수 생성\n",
    "    if {'최고가(원/kg)', '중간가(원/kg)', '최저가(원/kg)'}.issubset(filtered_data.columns):\n",
    "        filtered_data['변동폭(최고_최저)'] = filtered_data['최고가(원/kg)'] - filtered_data['최저가(원/kg)']\n",
    "        filtered_data['변동폭(평균_중간)'] = filtered_data['평균가(원/kg)'] - filtered_data['중간가(원/kg)']\n",
    "\n",
    "    # 표준편차 계산\n",
    "    filtered_data['시점_날짜'] = pd.to_datetime(filtered_data['시점'], errors='coerce')\n",
    "    if not filtered_data['시점_날짜'].isna().all():\n",
    "        monthly_std = filtered_data.groupby(filtered_data['시점_날짜'].dt.to_period('M'))[\n",
    "            ['최고가(원/kg)', '최저가(원/kg)', '평균가(원/kg)']\n",
    "        ].std().add_prefix('월_표준편차_')\n",
    "        filtered_data = filtered_data.merge(monthly_std, left_on=filtered_data['시점_날짜'].dt.to_period('M'), right_index=True, how='left')\n",
    "\n",
    "    # 품목별 선형회귀분석\n",
    "    if {'최고가(원/kg)', '중간가(원/kg)', '최저가(원/kg)', '평균가(원/kg)'}.issubset(filtered_data.columns):\n",
    "        regression_model = LinearRegression()\n",
    "        valid_rows = filtered_data.dropna(subset=['최고가(원/kg)', '중간가(원/kg)', '최저가(원/kg)', '평균가(원/kg)'])\n",
    "        X = valid_rows[['최고가(원/kg)', '중간가(원/kg)', '최저가(원/kg)']]\n",
    "        y = valid_rows['평균가(원/kg)']\n",
    "        if not X.empty:\n",
    "            regression_model.fit(X, y)\n",
    "            filtered_data['월_시세'] = (\n",
    "                regression_model.coef_[0] * filtered_data['최고가(원/kg)'] +\n",
    "                regression_model.coef_[1] * filtered_data['중간가(원/kg)'] +\n",
    "                regression_model.coef_[2] * filtered_data['최저가(원/kg)']\n",
    "            )\n",
    "        else:\n",
    "            filtered_data['월_시세'] = np.nan\n",
    "\n",
    "    # 데이터 타입 최적화\n",
    "    numeric_columns = filtered_data.select_dtypes(include=[np.number]).columns\n",
    "    filtered_data[numeric_columns] = filtered_data[numeric_columns].fillna(0).astype('float32')\n",
    "\n",
    "    # 정규화\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        filtered_data[numeric_columns] = scaler.fit_transform(filtered_data[numeric_columns])\n",
    "    else:\n",
    "        filtered_data[numeric_columns] = scaler.transform(filtered_data[numeric_columns])\n",
    "\n",
    "    return filtered_data, scaler\n",
    "\n",
    "# 학습 및 추론\n",
    "품목_리스트 = ['감자', '건고추', '깐마늘(국산)', '대파', '무', '배추', '사과', '상추', '양파', '배']\n",
    "거래단위 = {'감자': 20, '깐마늘': 20, '대파': 1, '무': 20, '배추': 10, '사과': 3, '배': 5, '상추': 0.1, '양파': 1}\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('predictions', exist_ok=True)\n",
    "\n",
    "품목별_predictions = {}\n",
    "품목별_scalers = {}\n",
    "\n",
    "pbar_outer = tqdm(품목_리스트, desc=\"품목 처리 중\")\n",
    "for 품목명 in pbar_outer:\n",
    "    # 데이터 전처리\n",
    "    train_data, scaler = process_data(\n",
    "        \"C:/Users/medici/pybasic/train.csv\",\n",
    "        \"C:/Users/medici/pybasic/TRAIN_산지공판장_2018-2021.csv\",\n",
    "        \"C:/Users/medici/pybasic/TRAIN_전국도매_2018-2021.csv\",\n",
    "        품목명, 거래단위)\n",
    "    품목별_scalers[품목명] = scaler\n",
    "\n",
    "    # Feature/Target 설정\n",
    "    features = train_data.drop(columns=['평균가격(원)', '시점']).columns\n",
    "    target = '평균가격(원)'\n",
    "\n",
    "    X = train_data[features].values\n",
    "    y = train_data[target].values\n",
    "\n",
    "    # Train/Test Split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # 모델 학습\n",
    "    model = XGBRegressor(n_estimators=50, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=True)\n",
    "\n",
    "    # 모델 저장\n",
    "    joblib.dump(model, f'models/best_model_{품목명}.pkl')\n",
    "\n",
    "    # 테스트 데이터 추론\n",
    "    predictions = []\n",
    "    for i in range(25):  # 테스트 데이터셋 처리\n",
    "        test_file = f\"C:/Users/medici/pybasic/test/TEST_{i:02d}.csv\"\n",
    "        산지공판장_file = f\"C:/Users/medici/pybasic/test/meta/TEST_산지공판장_{i:02d}.csv\"\n",
    "        전국도매_file = f\"C:/Users/medici/pybasic/test/meta/TEST_전국도매_{i:02d}.csv\"\n",
    "\n",
    "        test_data, _ = process_data(test_file, 산지공판장_file, 전국도매_file, 품목명, scaler=scaler)\n",
    "        test_features = test_data[features].values\n",
    "\n",
    "        # 예측\n",
    "        predictions.extend(model.predict(test_features))\n",
    "\n",
    "    품목별_predictions[품목명] = predictions\n",
    "\n",
    "# 결과 저장\n",
    "joblib.dump(품목별_predictions, 'predictions/품목별_predictions.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "\n",
    "# 1. 데이터 로드 함수\n",
    "def load_data(raw_file, 산지공판장_file, 전국도매_file):\n",
    "    return pd.read_csv(raw_file), pd.read_csv(산지공판장_file), pd.read_csv(전국도매_file)\n",
    "\n",
    "\n",
    "# 2. 가격 조정 함수\n",
    "def adjust_prices(data, conversion_factor):\n",
    "    price_columns = ['평균가(원/kg)', '최저가(원/kg)', '최고가(원/kg)', '중간가(원/kg)']\n",
    "    for col in price_columns:\n",
    "        if col in data.columns:\n",
    "            data[col] = data[col] * conversion_factor\n",
    "    return data\n",
    "\n",
    "\n",
    "# 3. 타겟 필터링 함수\n",
    "def filter_target_data(raw_data, 품목명, conditions):\n",
    "    if 품목명 in conditions and 'target' in conditions[품목명]:\n",
    "        raw_품목 = raw_data[raw_data['품목명'] == 품목명]\n",
    "        return raw_품목[conditions[품목명]['target'](raw_품목)]\n",
    "    return raw_data[raw_data['품목명'] == 품목명]\n",
    "\n",
    "\n",
    "# 4. 메타데이터 필터링 함수\n",
    "def filter_metadata_data(산지공판장, 전국도매, 품목명, conditions):\n",
    "    if 품목명 in conditions:\n",
    "        if conditions[품목명].get('공판장'):\n",
    "            for key, values in conditions[품목명]['공판장'].items():\n",
    "                산지공판장 = 산지공판장[산지공판장[key].isin(values)]\n",
    "        if conditions[품목명].get('도매'):\n",
    "            for key, values in conditions[품목명]['도매'].items():\n",
    "                전국도매 = 전국도매[전국도매[key].isin(values)]\n",
    "    return 산지공판장, 전국도매\n",
    "\n",
    "\n",
    "# 5. 데이터 병합 함수\n",
    "def merge_data(filtered_data, 산지공판장, 전국도매):\n",
    "    try:\n",
    "        filtered_공판장 = 산지공판장.add_prefix('공판장_').rename(columns={'공판장_시점': '시점'})\n",
    "        filtered_도매 = 전국도매.add_prefix('도매_').rename(columns={'도매_시점': '시점'})\n",
    "        filtered_data = filtered_data.merge(filtered_공판장, on='시점', how='left')\n",
    "        filtered_data = filtered_data.merge(filtered_도매, on='시점', how='left')\n",
    "    except KeyError as e:\n",
    "        print(f\"병합 중 오류 발생: {e}\")\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "# 6. 파생 변수 생성 함수\n",
    "def generate_features(filtered_data):\n",
    "    if {'최고가(원/kg)', '중간가(원/kg)', '최저가(원/kg)'}.issubset(filtered_data.columns):\n",
    "        filtered_data['변동폭(최고_최저)'] = filtered_data['최고가(원/kg)'] - filtered_data['최저가(원/kg)']\n",
    "        filtered_data['변동폭(평균_중간)'] = filtered_data['평균가(원/kg)'] - filtered_data['중간가(원/kg)']\n",
    "\n",
    "    filtered_data['시점_날짜'] = pd.to_datetime(filtered_data['시점'], errors='coerce')\n",
    "    if not filtered_data['시점_날짜'].isna().all():\n",
    "        monthly_std = filtered_data.groupby(filtered_data['시점_날짜'].dt.to_period('M'))[\n",
    "            ['최고가(원/kg)', '최저가(원/kg)', '평균가(원/kg)']\n",
    "        ].std().add_prefix('월_표준편차_')\n",
    "        filtered_data = filtered_data.merge(monthly_std, left_on=filtered_data['시점_날짜'].dt.to_period('M'), right_index=True, how='left')\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "# 7. 선형 회귀 분석 함수\n",
    "def add_regression_feature(filtered_data):\n",
    "    if {'최고가(원/kg)', '중간가(원/kg)', '최저가(원/kg)', '평균가(원/kg)'}.issubset(filtered_data.columns):\n",
    "        regression_model = LinearRegression()\n",
    "        valid_rows = filtered_data.dropna(subset=['최고가(원/kg)', '중간가(원/kg)', '최저가(원/kg)', '평균가(원/kg)'])\n",
    "        X = valid_rows[['최고가(원/kg)', '중간가(원/kg)', '최저가(원/kg)']]\n",
    "        y = valid_rows['평균가(원/kg)']\n",
    "        if not X.empty:\n",
    "            regression_model.fit(X, y)\n",
    "            filtered_data['월_시세'] = (\n",
    "                regression_model.coef_[0] * filtered_data['최고가(원/kg)'] +\n",
    "                regression_model.coef_[1] * filtered_data['중간가(원/kg)'] +\n",
    "                regression_model.coef_[2] * filtered_data['최저가(원/kg)']\n",
    "            )\n",
    "        else:\n",
    "            filtered_data['월_시세'] = np.nan\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "# 8. 데이터 정규화 함수\n",
    "def preprocess_data(filtered_data, scaler=None):\n",
    "    numeric_columns = filtered_data.select_dtypes(include=[np.number]).columns\n",
    "    filtered_data[numeric_columns] = filtered_data[numeric_columns].fillna(0).astype('float32')\n",
    "\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        filtered_data[numeric_columns] = scaler.fit_transform(filtered_data[numeric_columns])\n",
    "    else:\n",
    "        filtered_data[numeric_columns] = scaler.transform(filtered_data[numeric_columns])\n",
    "\n",
    "    return filtered_data, scaler\n",
    "\n",
    "\n",
    "# 9. 모델 학습 함수\n",
    "def train_model(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"그리드 서치를 통한 모델 학습.\"\"\"\n",
    "    model = XGBRegressor(tree_method='gpu_hist', predictor='gpu_predictor', random_state=42)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': range(100, 1100, 200),  # [100, 300, 500, 700, 900]\n",
    "        'learning_rate': [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2],  # 매우 낮은 학습률부터 높은 학습률까지\n",
    "        'max_depth': range(3, 11),  # [3, 4, 5, ..., 10] (트리 깊이: 얕은 트리부터 깊은 트리까지)\n",
    "        'subsample': [i / 10.0 for i in range(5, 11)],  # [0.5, 0.6, ..., 1.0]\n",
    "        'colsample_bytree': [i / 10.0 for i in range(5, 11)]  # [0.5, 0.6, ..., 1.0]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        cv=3,\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "        error_score='raise'  # 정확한 오류 추적\n",
    "    )\n",
    "    # 그리드서치 실행\n",
    "    grid_search.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=True)\n",
    "\n",
    "    # 최적의 하이퍼파라미터와 성능 출력\n",
    "    print(f\"최적의 하이퍼파라미터: {grid_search.best_params_}\")\n",
    "    print(f\"최적의 성능: {-grid_search.best_score_:.4f}\")\n",
    "\n",
    "    # 최적의 모델 반환\n",
    "    best_model = grid_search.best_estimator_\n",
    "    return best_model\n",
    "\n",
    "\n",
    "# 10. 전체 워크플로 함수\n",
    "def process_workflow(품목_리스트, 거래단위, conditions, train_files, test_files, output_file):\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "\n",
    "    품목별_predictions = {}\n",
    "    pbar_outer = tqdm(품목_리스트, desc=\"품목 처리 중\")\n",
    "    for 품목명 in pbar_outer:\n",
    "        try:\n",
    "            raw_data, 산지공판장, 전국도매 = load_data(*train_files)\n",
    "\n",
    "            if 품목명 in 거래단위:\n",
    "                factor = 거래단위[품목명]\n",
    "                산지공판장 = adjust_prices(산지공판장, factor)\n",
    "                전국도매 = adjust_prices(전국도매, factor)\n",
    "\n",
    "            filtered_data = filter_target_data(raw_data, 품목명, conditions)\n",
    "            산지공판장, 전국도매 = filter_metadata_data(산지공판장, 전국도매, 품목명, conditions)\n",
    "            filtered_data = merge_data(filtered_data, 산지공판장, 전국도매)\n",
    "\n",
    "            filtered_data = generate_features(filtered_data)\n",
    "            filtered_data = add_regression_feature(filtered_data)\n",
    "            filtered_data, scaler = preprocess_data(filtered_data)\n",
    "\n",
    "            features = filtered_data.drop(columns=['평균가격(원)', '시점']).columns\n",
    "            target = '평균가격(원)'\n",
    "            X = filtered_data[features].values\n",
    "            y = filtered_data[target].values\n",
    "\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            model = train_model(X_train, y_train, X_val, y_val)\n",
    "\n",
    "            joblib.dump(model, f'models/best_model_{품목명}.pkl')\n",
    "\n",
    "            # 테스트 데이터 추론\n",
    "            predictions = []\n",
    "            for test_file in test_files:\n",
    "                test_data, _ = preprocess_data(pd.read_csv(test_file), scaler=scaler)\n",
    "                test_features = test_data[features].values\n",
    "                predictions.extend(model.predict(test_features))\n",
    "\n",
    "            품목별_predictions[품목명] = predictions\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"{품목명} 처리 중 오류 발생: {e}\")\n",
    "\n",
    "    sample_submission = pd.read_csv(output_file)\n",
    "    for 품목명, predictions in 품목별_predictions.items():\n",
    "        sample_submission[품목명] = predictions\n",
    "    sample_submission.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "품목 처리 중:   0%|          | 0/10 [00:00<?, ?it/s]C:\\Users\\medici\\AppData\\Local\\Temp\\ipykernel_25852\\1141033359.py:64: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  filtered_data['시점_날짜'] = pd.to_datetime(filtered_data['시점'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8640 candidates, totalling 25920 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "품목 처리 중:  10%|█         | 1/10 [00:05<00:52,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감자 처리 중 오류 발생: could not convert string to float: '감자'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "품목 처리 중:  20%|██        | 2/10 [00:24<01:47, 13.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "건고추 처리 중 오류 발생: Unable to allocate 6.65 GiB for an array with shape (6, 148691282) and data type int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\medici\\AppData\\Local\\Temp\\ipykernel_25852\\1141033359.py:64: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  filtered_data['시점_날짜'] = pd.to_datetime(filtered_data['시점'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8640 candidates, totalling 25920 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "품목 처리 중:  30%|███       | 3/10 [00:30<01:10, 10.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "깐마늘(국산) 처리 중 오류 발생: could not convert string to float: '깐마늘(국산)'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\medici\\AppData\\Local\\Temp\\ipykernel_25852\\1141033359.py:64: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  filtered_data['시점_날짜'] = pd.to_datetime(filtered_data['시점'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8640 candidates, totalling 25920 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "품목 처리 중:  40%|████      | 4/10 [00:36<00:50,  8.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대파 처리 중 오류 발생: could not convert string to float: '대파'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\medici\\AppData\\Local\\Temp\\ipykernel_25852\\1141033359.py:64: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  filtered_data['시점_날짜'] = pd.to_datetime(filtered_data['시점'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8640 candidates, totalling 25920 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "품목 처리 중:  50%|█████     | 5/10 [00:42<00:37,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "무 처리 중 오류 발생: could not convert string to float: '무'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\medici\\AppData\\Local\\Temp\\ipykernel_25852\\1141033359.py:64: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  filtered_data['시점_날짜'] = pd.to_datetime(filtered_data['시점'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8640 candidates, totalling 25920 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "품목 처리 중:  60%|██████    | 6/10 [01:40<01:38, 24.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배추 처리 중 오류 발생: could not convert string to float: '배추'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\medici\\AppData\\Local\\Temp\\ipykernel_25852\\1141033359.py:64: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  filtered_data['시점_날짜'] = pd.to_datetime(filtered_data['시점'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8640 candidates, totalling 25920 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "품목 처리 중:  70%|███████   | 7/10 [01:46<00:55, 18.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사과 처리 중 오류 발생: could not convert string to float: '사과'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\medici\\AppData\\Local\\Temp\\ipykernel_25852\\1141033359.py:64: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  filtered_data['시점_날짜'] = pd.to_datetime(filtered_data['시점'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8640 candidates, totalling 25920 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "품목 처리 중:  80%|████████  | 8/10 [01:52<00:29, 14.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "상추 처리 중 오류 발생: could not convert string to float: '상추'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\medici\\AppData\\Local\\Temp\\ipykernel_25852\\1141033359.py:64: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  filtered_data['시점_날짜'] = pd.to_datetime(filtered_data['시점'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8640 candidates, totalling 25920 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "품목 처리 중:  90%|█████████ | 9/10 [01:58<00:11, 11.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "양파 처리 중 오류 발생: could not convert string to float: '양파'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\medici\\AppData\\Local\\Temp\\ipykernel_25852\\1141033359.py:64: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  filtered_data['시점_날짜'] = pd.to_datetime(filtered_data['시점'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8640 candidates, totalling 25920 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "품목 처리 중: 100%|██████████| 10/10 [02:04<00:00, 12.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배 처리 중 오류 발생: could not convert string to float: '배'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 데이터 경로 설정\n",
    "    train_raw_file = \"C:/Users/medici/pybasic/train.csv\"\n",
    "    train_산지공판장_file = \"C:/Users/medici/pybasic/TRAIN_산지공판장_2018-2021.csv\"\n",
    "    train_전국도매_file = \"C:/Users/medici/pybasic/TRAIN_전국도매_2018-2021.csv\"\n",
    "    test_files = [f\"C:/Users/medici/pybasic/test/TEST_{i:02d}.csv\" for i in range(25)]\n",
    "    test_산지공판장_files = [f\"C:/Users/medici/pybasic/test/meta/TEST_산지공판장_{i:02d}.csv\" for i in range(25)]\n",
    "    test_전국도매_files = [f\"C:/Users/medici/pybasic/test/meta/TEST_전국도매_{i:02d}.csv\" for i in range(25)]\n",
    "    output_file = \"C:/Users/medici/pybasic/baseline_submission_origin.csv\"\n",
    "\n",
    "    # 품목 리스트 및 거래단위\n",
    "    품목_리스트 = ['감자', '건고추', '깐마늘(국산)', '대파', '무', '배추', '사과', '상추', '양파', '배']\n",
    "    거래단위 = {'감자': 20, '깐마늘(국산)': 20, '대파': 1, '무': 20, '배추': 10, '사과': 3, '배': 5, '상추': 0.1, '양파': 1}\n",
    "\n",
    "    # 타겟 조건\n",
    "    conditions = {\n",
    "        '감자': {\n",
    "            'target': lambda df: (df['품종명'] == '감자 수미') & (df['거래단위'] == '20키로상자') & (df['등급'] == '상'),\n",
    "            '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['감자'], '품종명': ['수미'], '등급명': ['상']},\n",
    "            '도매': {'시장명': ['*전국도매시장'], '품목명': ['감자'], '품종명': ['수미']}\n",
    "        },\n",
    "        '건고추': {\n",
    "            'target': lambda df: (df['품종명'] == '화건') & (df['거래단위'] == '30 kg') & (df['등급'] == '상품'),\n",
    "            '공판장': None, \n",
    "            '도매': None  \n",
    "        },\n",
    "        '깐마늘(국산)': {\n",
    "            'target': lambda df: (df['거래단위'] == '20 kg') & (df['등급'] == '상품'),\n",
    "            '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['마늘'], '품종명': ['깐마늘'], '등급명': ['상']},\n",
    "            '도매': {'시장명': ['*전국도매시장'], '품목명': ['마늘'], '품종명': ['깐마늘']}\n",
    "        },\n",
    "        '대파': {\n",
    "            'target': lambda df: (df['품종명'] == '대파(일반)') & (df['거래단위'] == '1키로단') & (df['등급'] == '상'),\n",
    "            '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['대파'], '품종명': ['대파(일반)'], '등급명': ['상']},\n",
    "            '도매': {'시장명': ['*전국도매시장'], '품목명': ['대파'], '품종명': ['대파(일반)']}\n",
    "        },\n",
    "        '무': {\n",
    "            'target': lambda df: (df['거래단위'] == '20키로상자') & (df['등급'] == '상'),\n",
    "            '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['무'], '품종명': ['기타무'], '등급명': ['상']},\n",
    "            '도매': {'시장명': ['*전국도매시장'], '품목명': ['무'], '품종명': ['무']}\n",
    "        },\n",
    "        '배추': {\n",
    "            'target': lambda df: (df['거래단위'] == '10키로망대') & (df['등급'] == '상'),\n",
    "            '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['배추'], '품종명': ['쌈배추'], '등급명': ['상']},\n",
    "            '도매': {'시장명': ['*전국도매시장'], '품목명': ['배추'], '품종명': ['배추']}\n",
    "        },\n",
    "        '사과': {\n",
    "            'target': lambda df: (df['품종명'].isin(['홍로', '후지'])) & (df['거래단위'] == '10 개') & (df['등급'] == '상품'),\n",
    "            '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['사과'], '품종명': ['후지'], '등급명': ['상']},\n",
    "            '도매': {'시장명': ['*전국도매시장'], '품목명': ['사과'], '품종명': ['후지']}\n",
    "        },\n",
    "        '상추': {\n",
    "            'target': lambda df: (df['품종명'] == '청') & (df['거래단위'] == '100 g') & (df['등급'] == '상품'),\n",
    "            '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['상추'], '품종명': ['청상추'], '등급명': ['상']},\n",
    "            '도매': {'시장명': ['*전국도매시장'], '품목명': ['상추'], '품종명': ['청상추']}\n",
    "        },\n",
    "        '양파': {\n",
    "            'target': lambda df: (df['품종명'] == '양파') & (df['거래단위'] == '1키로') & (df['등급'] == '상'),\n",
    "            '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['양파'], '품종명': ['기타양파'], '등급명': ['상']},\n",
    "            '도매': {'시장명': ['*전국도매시장'], '품목명': ['양파'], '품종명': ['양파(일반)']}\n",
    "        },\n",
    "        '배': {\n",
    "            'target': lambda df: (df['품종명'] == '신고') & (df['거래단위'] == '10 개') & (df['등급'] == '상품'),\n",
    "            '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['배'], '품종명': ['신고'], '등급명': ['상']},\n",
    "            '도매': {'시장명': ['*전국도매시장'], '품목명': ['배'], '품종명': ['신고']}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    train_files = (train_raw_file, train_산지공판장_file, train_전국도매_file)\n",
    "    process_workflow(품목_리스트, 거래단위, conditions, train_files, test_files, output_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syrup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
